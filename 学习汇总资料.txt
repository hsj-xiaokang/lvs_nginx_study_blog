经典方式介绍【使用LVS实现负载均衡原理及安装配置详解】：https://www.cnblogs.com/liwei0526vip/p/6370103.html
使用LVS实现负载均衡原理及安装配置详解

    负载均衡集群是 load balance 集群的简写，翻译成中文就是负载均衡集群。常用的负载均衡开源软件有nginx、lvs、haproxy，商业的硬件负载均衡设备F5、Netscale。这里主要是学习 LVS 并对其进行了详细的总结记录。

一、负载均衡LVS基本介绍
    LB集群的架构和原理很简单，就是当用户的请求过来时，会直接分发到Director Server上，然后它把用户的请求根据设置好的调度算法，智能均衡地分发到后端真正服务器(real server)上。为了避免不同机器上用户请求得到的数据不一样，需要用到了共享存储，这样保证所有用户请求的数据是一样的。

    LVS是 Linux Virtual Server 的简称，也就是Linux虚拟服务器。这是一个由章文嵩博士发起的一个开源项目，它的官方网站是 http://www.linuxvirtualserver.org 现在 LVS 已经是 Linux 内核标准的一部分。使用 LVS 可以达到的技术目标是：通过 LVS 达到的负载均衡技术和 Linux 操作系统实现一个高性能高可用的 Linux 服务器集群，它具有良好的可靠性、可扩展性和可操作性。从而以低廉的成本实现最优的性能。LVS 是一个实现负载均衡集群的开源软件项目，LVS架构从逻辑上可分为调度层、Server集群层和共享存储。

 

二、LVS的基本工作原理
###################################################图一#############################################################

1. 当用户向负载均衡调度器（Director Server）发起请求，调度器将请求发往至内核空间
2. PREROUTING链首先会接收到用户请求，判断目标IP确定是本机IP，将数据包发往INPUT链
3. IPVS是工作在INPUT链上的，当用户请求到达INPUT时，IPVS会将用户请求和自己已定义好的集群服务进行比对，如果用户请求的就是定义的集群服务，那么此时IPVS会强行修改数据包里的目标IP地址及端口，并将新的数据包发往POSTROUTING链
4. POSTROUTING链接收数据包后发现目标IP地址刚好是自己的后端服务器，那么此时通过选路，将数据包最终发送给后端的服务器

 

三、LVS的组成
LVS 由2部分程序组成，包括 ipvs 和 ipvsadm。

1. ipvs(ip virtual server)：一段代码工作在内核空间，叫ipvs，是真正生效实现调度的代码。
2. ipvsadm：另外一段是工作在用户空间，叫ipvsadm，负责为ipvs内核框架编写规则，定义谁是集群服务，而谁是后端真实的服务器(Real Server)

 

四、LVS相关术语
1. DS：Director Server。指的是前端负载均衡器节点。
2. RS：Real Server。后端真实的工作服务器。
3. VIP：向外部直接面向用户请求，作为用户请求的目标的IP地址。
4. DIP：Director Server IP，主要用于和内部主机通讯的IP地址。
5. RIP：Real Server IP，后端服务器的IP地址。
6. CIP：Client IP，访问客户端的IP地址。

下边是三种工作模式的原理和特点总结。

 

五、LVS/NAT原理和特点
1. 重点理解NAT方式的实现原理和数据包的改变。

###################################################图二#############################################################

(a). 当用户请求到达Director Server，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP 
(b). PREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链
(c). IPVS比对数据包请求的服务是否为集群服务，若是，修改数据包的目标IP地址为后端服务器IP，然后将数据包发至POSTROUTING链。 此时报文的源IP为CIP，目标IP为RIP 
(d). POSTROUTING链通过选路，将数据包发送给Real Server
(e). Real Server比对发现目标为自己的IP，开始构建响应报文发回给Director Server。 此时报文的源IP为RIP，目标IP为CIP 
(f). Director Server在响应客户端前，此时会将源IP地址修改为自己的VIP地址，然后响应给客户端。 此时报文的源IP为VIP，目标IP为CIP

2. LVS-NAT模型的特性

RS应该使用私有地址，RS的网关必须指向DIP
DIP和RIP必须在同一个网段内
请求和响应报文都需要经过Director Server，高负载场景中，Director Server易成为性能瓶颈
支持端口映射
RS可以使用任意操作系统
缺陷：对Director Server压力会比较大，请求和响应都需经过director server
 

六、LVS/DR原理和特点
1. 重将请求报文的目标MAC地址设定为挑选出的RS的MAC地址

 ###################################################图三#############################################################

(a) 当用户请求到达Director Server，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP
(b) PREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链
(c) IPVS比对数据包请求的服务是否为集群服务，若是，将请求报文中的源MAC地址修改为DIP的MAC地址，将目标MAC地址修改RIP的MAC地址，然后将数据包发至POSTROUTING链。 此时的源IP和目的IP均未修改，仅修改了源MAC地址为DIP的MAC地址，目标MAC地址为RIP的MAC地址 
(d) 由于DS和RS在同一个网络中，所以是通过二层来传输。POSTROUTING链检查目标MAC地址为RIP的MAC地址，那么此时数据包将会发至Real Server。
(e) RS发现请求报文的MAC地址是自己的MAC地址，就接收此报文。处理完成之后，将响应报文通过lo接口传送给eth0网卡然后向外发出。 此时的源IP地址为VIP，目标IP为CIP 
(f) 响应报文最终送达至客户端

2. LVS-DR模型的特性

特点1：保证前端路由将目标地址为VIP报文统统发给Director Server，而不是RS
RS可以使用私有地址；也可以是公网地址，如果使用公网地址，此时可以通过互联网对RIP进行直接访问
RS跟Director Server必须在同一个物理网络中
所有的请求报文经由Director Server，但响应报文必须不能进过Director Server
不支持地址转换，也不支持端口映射
RS可以是大多数常见的操作系统
RS的网关绝不允许指向DIP(因为我们不允许他经过director)
RS上的lo接口配置VIP的IP地址
缺陷：RS和DS必须在同一机房中
3. 特点1的解决方案：

在前端路由器做静态地址路由绑定，将对于VIP的地址仅路由到Director Server
存在问题：用户未必有路由操作权限，因为有可能是运营商提供的，所以这个方法未必实用
arptables：在arp的层次上实现在ARP解析时做防火墙规则，过滤RS响应ARP请求。这是由iptables提供的
修改RS上内核参数（arp_ignore和arp_announce）将RS上的VIP配置在lo接口的别名上，并限制其不能响应对VIP地址解析请求。
 

七、LVS/Tun原理和特点
在原有的IP报文外再次封装多一层IP首部，内部IP首部(源地址为CIP，目标IIP为VIP)，外层IP首部(源地址为DIP，目标IP为RIP)
###################################################图四#############################################################


(a) 当用户请求到达Director Server，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP 。
(b) PREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链
(c) IPVS比对数据包请求的服务是否为集群服务，若是，在请求报文的首部再次封装一层IP报文，封装源IP为为DIP，目标IP为RIP。然后发至POSTROUTING链。 此时源IP为DIP，目标IP为RIP 
(d) POSTROUTING链根据最新封装的IP报文，将数据包发至RS（因为在外层封装多了一层IP首部，所以可以理解为此时通过隧道传输）。 此时源IP为DIP，目标IP为RIP
(e) RS接收到报文后发现是自己的IP地址，就将报文接收下来，拆除掉最外层的IP后，会发现里面还有一层IP首部，而且目标是自己的lo接口VIP，那么此时RS开始处理此请求，处理完成之后，通过lo接口送给eth0网卡，然后向外传递。 此时的源IP地址为VIP，目标IP为CIP
(f) 响应报文最终送达至客户端

LVS-Tun模型特性

RIP、VIP、DIP全是公网地址
RS的网关不会也不可能指向DIP
所有的请求报文经由Director Server，但响应报文必须不能进过Director Server
不支持端口映射
RS的系统必须支持隧道
其实企业中最常用的是 DR 实现方式，而 NAT 配置上比较简单和方便，后边实践中会总结 DR 和 NAT 具体使用配置过程。

 

八、LVS的八种调度算法
1. 轮叫调度 rr
这种算法是最简单的，就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是简单。轮询算法假设所有的服务器处理请求的能力都是一样的，调度器会将所有的请求平均分配给每个真实服务器，不管后端 RS 配置和处理能力，非常均衡地分发下去。

2. 加权轮叫 wrr
这种算法比 rr 的算法多了一个权重的概念，可以给 RS 设置权重，权重越高，那么分发的请求数越多，权重的取值范围 0 – 100。主要是对rr算法的一种优化和补充， LVS 会考虑每台服务器的性能，并给每台服务器添加要给权值，如果服务器A的权值为1，服务器B的权值为2，则调度到服务器B的请求会是服务器A的2倍。权值越高的服务器，处理的请求越多。

3. 最少链接 lc
这个算法会根据后端 RS 的连接数来决定把请求分发给谁，比如 RS1 连接数比 RS2 连接数少，那么请求就优先发给 RS1 

4. 加权最少链接 wlc
这个算法比 lc 多了一个权重的概念。

5. 基于局部性的最少连接调度算法 lblc
这个算法是请求数据包的目标 IP 地址的一种调度算法，该算法先根据请求的目标 IP 地址寻找最近的该目标 IP 地址所有使用的服务器，如果这台服务器依然可用，并且有能力处理该请求，调度器会尽量选择相同的服务器，否则会继续选择其它可行的服务器

6. 复杂的基于局部性最少的连接算法 lblcr
记录的不是要给目标 IP 与一台服务器之间的连接记录，它会维护一个目标 IP 到一组服务器之间的映射关系，防止单点服务器负载过高。

7. 目标地址散列调度算法 dh
该算法是根据目标 IP 地址通过散列函数将目标 IP 与服务器建立映射关系，出现服务器不可用或负载过高的情况下，发往该目标 IP 的请求会固定发给该服务器。

8. 源地址散列调度算法 sh
与目标地址散列调度算法类似，但它是根据源地址散列算法进行静态分配固定的服务器资源。

 

九、实践LVS的NAT模式
1、实验环境

三台服务器，一台作为 director，两台作为 real server，director 有一个外网网卡(172.16.254.200) 和一个内网ip(192.168.0.8)，两个 real server 上只有内网 ip (192.168.0.18) 和 (192.168.0.28)，并且需要把两个 real server 的内网网关设置为 director 的内网 ip(192.168.0.8)

2、安装和配置

两个 real server 上都安装 nginx 服务
# yum install -y nginx

Director 上安装 ipvsadm
# yum install -y ipvsadm
Director 上编辑 nat 实现脚本

复制代码
# vim /usr/local/sbin/lvs_nat.sh
# 编辑写入如下内容：
#! /bin/bash
# director服务器上开启路由转发功能:
echo 1 > /proc/sys/net/ipv4/ip_forward
# 关闭 icmp 的重定向
echo 0 > /proc/sys/net/ipv4/conf/all/send_redirects
echo 0 > /proc/sys/net/ipv4/conf/default/send_redirects
echo 0 > /proc/sys/net/ipv4/conf/eth0/send_redirects
echo 0 > /proc/sys/net/ipv4/conf/eth1/send_redirects
# director设置 nat 防火墙
iptables -t nat -F
iptables -t nat -X
iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j MASQUERADE
# director设置 ipvsadm
IPVSADM='/sbin/ipvsadm'
$IPVSADM -C
$IPVSADM -A -t 172.16.254.200:80 -s wrr
$IPVSADM -a -t 172.16.254.200:80 -r 192.168.0.18:80 -m -w 1
$IPVSADM -a -t 172.16.254.200:80 -r 192.168.0.28:80 -m -w 1
复制代码
保存后，在 Director 上直接运行这个脚本就可以完成 lvs/nat 的配置

/bin/bash /usr/local/sbin/lvs_nat.sh
查看ipvsadm设置的规则

ipvsadm -ln
3、测试LVS的效果

通过浏览器测试2台机器上的web内容 http://172.16.254.200 。为了区分开，我们可以把 nginx 的默认页修改一下：

在 RS1 上执行
# echo "rs1rs1" >/usr/share/nginx/html/index.html

在 RS2 上执行
# echo "rs2rs2" >/usr/share/nginx/html/index.html
注意，切记一定要在两台 RS 上设置网关的 IP 为 director 的内网 IP。

 

十、实践LVS的DR模式
1、实验环境

三台机器：

Director节点：  (eth0 192.168.0.8  vip eth0:0 192.168.0.38)
Real server1： (eth0 192.168.0.18 vip lo:0 192.168.0.38)
Real server2： (eth0 192.168.0.28 vip lo:0 192.168.0.38)
2、安装

两个 real server 上都安装 nginx 服务
# yum install -y nginx

Director 上安装 ipvsadm
# yum install -y ipvsadm
3、Director 上配置脚本

复制代码
# vim /usr/local/sbin/lvs_dr.sh
#! /bin/bash
echo 1 > /proc/sys/net/ipv4/ip_forward
ipv=/sbin/ipvsadm
vip=192.168.0.38
rs1=192.168.0.18
rs2=192.168.0.28
ifconfig eth0:0 down
ifconfig eth0:0 $vip broadcast $vip netmask 255.255.255.255 up
route add -host $vip dev eth0:0
$ipv -C
$ipv -A -t $vip:80 -s wrr 
$ipv -a -t $vip:80 -r $rs1:80 -g -w 3
$ipv -a -t $vip:80 -r $rs2:80 -g -w 1
复制代码
执行脚本：

# bash /usr/local/sbin/lvs_dr.sh
4、在2台 rs 上配置脚本：

复制代码
# vim /usr/local/sbin/lvs_dr_rs.sh
#! /bin/bash
vip=192.168.0.38
ifconfig lo:0 $vip broadcast $vip netmask 255.255.255.255 up
route add -host $vip lo:0
echo "1" >/proc/sys/net/ipv4/conf/lo/arp_ignore
echo "2" >/proc/sys/net/ipv4/conf/lo/arp_announce
echo "1" >/proc/sys/net/ipv4/conf/all/arp_ignore
echo "2" >/proc/sys/net/ipv4/conf/all/arp_announce
复制代码
rs 上分别执行脚本：

bash /usr/local/sbin/lvs_dr_rs.sh
5、实验测试

测试方式同上，浏览器访问 http://192.168.0.38

注意：在 DR 模式下，2台 rs 节点的 gateway 不需要设置成 dir 节点的 IP 。

参考链接地址：http://www.cnblogs.com/lgfeng/archive/2012/10/16/2726308.html

 

十一、LVS结合keepalive
LVS可以实现负载均衡，但是不能够进行健康检查，比如一个rs出现故障，LVS 仍然会把请求转发给故障的rs服务器，这样就会导致请求的无效性。keepalive 软件可以进行健康检查，而且能同时实现 LVS 的高可用性，解决 LVS 单点故障的问题，其实 keepalive 就是为 LVS 而生的。

1、实验环境

4台节点

Keepalived1 + lvs1(Director1)：192.168.0.48
Keepalived2 + lvs2(Director2)：192.168.0.58
Real server1：192.168.0.18
Real server2：192.168.0.28
IP: 192.168.0.38
2、安装系统软件

Lvs + keepalived的2个节点安装

# yum install ipvsadm keepalived -y
Real server + nginx服务的2个节点安装

# yum install epel-release -y
# yum install nginx -y
3、设置配置脚本

Real server节点2台配置脚本：

复制代码
# vim /usr/local/sbin/lvs_dr_rs.sh
#! /bin/bash
vip=192.168.0.38
ifconfig lo:0 $vip broadcast $vip netmask 255.255.255.255 up
route add -host $vip lo:0
echo "1" >/proc/sys/net/ipv4/conf/lo/arp_ignore
echo "2" >/proc/sys/net/ipv4/conf/lo/arp_announce
echo "1" >/proc/sys/net/ipv4/conf/all/arp_ignore
echo "2" >/proc/sys/net/ipv4/conf/all/arp_announce

2节点rs 上分别执行脚本：
bash /usr/local/sbin/lvs_dr_rs.sh
复制代码
keepalived节点配置(2节点)：

复制代码
主节点( MASTER )配置文件
vim /etc/keepalived/keepalived.conf
vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.0.38
    }
}

virtual_server 192.168.0.38 80 {
    delay_loop 6
    lb_algo rr
    lb_kind DR
    persistence_timeout 0
    protocol TCP

    real_server 192.168.0.18 80 {
        weight 1
        TCP_CHECK {
            connect_timeout 10
            nb_get_retry 3
            delay_before_retry 3
            connect_port 80
        }
    }

    real_server 192.168.0.28 80 {
        weight 1
        TCP_CHECK {
            connect_timeout 10
            nb_get_retry 3
            delay_before_retry 3
            connect_port 80
        }
    }
}
复制代码
从节点( BACKUP )配置文件

拷贝主节点的配置文件keepalived.conf，然后修改如下内容：

state MASTER -> state BACKUP
priority 100 -> priority 90
keepalived的2个节点执行如下命令，开启转发功能：

# echo 1 > /proc/sys/net/ipv4/ip_forward
4、启动keepalive

先主后从分别启动keepalive
service keepalived start
5、验证结果

实验1

手动关闭192.168.0.18节点的nginx，service nginx stop 在客户端上去测试访问 http://192.168.0.38 结果正常，不会出现访问18节点，一直访问的是28节点的内容。

实验2

手动重新开启 192.168.0.18 节点的nginx， service nginx start 在客户端上去测试访问 http://192.168.0.38 结果正常，按照 rr 调度算法访问18节点和28节点。

实验3

测试 keepalived 的HA特性，首先在master上执行命令 ip addr ，可以看到38的vip在master节点上的；这时如果在master上执行 service keepalived stop 命令，这时vip已经不再master上，在slave节点上执行 ip addr 命令可以看到 vip 已经正确漂到slave节点，这时客户端去访问 http://192.168.0.38 访问依然正常，验证了 keepalived的HA特性。

lvs 介绍：http://www.it165.net/admin/html/201401/2248.html

 

作者：liwei0526vip 出处：http://www.cnblogs.com/liwei0526vip 欢迎转载，也请保留这段声明。谢谢！




















关于LVS+Nginx为什么会被同时使用的思考  http://blog.csdn.net/BuquTianya/article/details/52076153

最初的理解

(也可以每个nginx都挂在上所有的应用服务器) 
nginx大家都在用，估计也很熟悉了，在做负载均衡时很好用，安装简单、配置简单、相关材料也特别多。

lvs是国内的章文嵩博士的大作，比nginx被广泛接受还要早7年，并且已经被红帽作为了系统内置软件，可谓很牛了。lvs相对于nginx来说配置上就要相对复杂一些。

但是，有时候我们会看到大牛们分享的经验里面是lvs+nginx作为负载均衡了，一直想不明白这是个什么道理。

为什么会出现两者被同时使用呢？其实，这要从两者的各自优势来说了。

nginx用来做http的反向代理，能够upsteam实现http请求的多种方式的均衡转发。由于采用的是异步转发可以做到如果一个服务器请求失败，立即切换到其他服务器，直到请求成功或者最后一台服务器失败为止。这可以最大程度的提高系统的请求成功率。

lvs采用的是同步请求转发的策略。这里说一下同步转发和异步转发的区别。同步转发是在lvs服务器接收到请求之后，立即redirect到一个后端服务器，由客户端直接和后端服务器建立连接。异步转发是nginx在保持客户端连接的同时，发起一个相同内容的新请求到后端，等后端返回结果后，由nginx返回给客户端。

进一步来说：当做为负载均衡服务器的nginx和lvs处理相同的请求时，所有的请求和响应流量都会经过nginx；但是使用lvs时，仅请求流量经过lvs的网络，响应流量由后端服务器的网络返回。

也就是，当作为后端的服务器规模庞大时，nginx的网络带宽就成了一个巨大的瓶颈。

但是仅仅使用lvs作为负载均衡的话，一旦后端接受到请求的服务器出了问题，那么这次请求就失败了。但是如果在lvs的后端在添加一层nginx（多个），每个nginx后端再有几台应用服务器，那么结合两者的优势，既能避免单nginx的流量集中瓶颈，又能避免单lvs时一锤子买卖的问题。



补充（20160731 下午2：07）：

在后续继续了解这部分内容时，发现了这两个帖子： 
1. 有了LVS，还要apache，nginx有什么用？ 
2.从一个开发的角度看负载均衡和LVS—–注意看最后一个lvs集群化的图，nginx和rs是一对一连接的。

从以上文章来看，lvs+nginx组合使用的原因主要是用lvs来做负载均衡（因为工作在4层，效率高），nginx来做静态文件的处理。

这里第二篇文章，和lvs的后续维护者之一的[吴佳明_普空的ppt](http://velocity.oreilly.com.cn/2012/ppts/pukong.pdf 
)中比章博士的文章中多出了一种转发模式，也就是full_nat模式，这种模式下，所有的响应也要经过lvs服务器作为出口返回给客户端。

lvs在full_nat的模式下，是否还是同步的模式呢？我猜想应该是的，因为lvs工作在4层，所以可能当前出错的响应要映射到之前的那一次请求（因为没有解析http包），所以也就做不到把错误转发到其他的应用服务器上重新处理。

另外，这个补充是不是就完全的否定了昨天（也就是补充之前的那一大段）的内容呢？我觉得并没有。昨天的理解可能不是正规大型互联网在使用的模式（当然，这里我不能确定，因为我没有大型互联网的经验），但是理论应该是对的。

具体一点，lvs+nginx的组合nginx用来做静态文件的处理的场景下，如果一次请求失败了，那么久不能被重新处理了，当然你的nginx也可以后挂1个以上的应用服务器（这样的话nginx实质上也是起到了lvs均衡补充的效果）。另外就是，nginx不是单单的作为静态文件的处理，而是作为lvs的一个补充，互相弥补均衡上的不足。

那么你可能会说lvs在4层上处理的高效就不复存在了，确实是的，这点要承认，但是，我们提高了系统请求的成功率，两者需要各自去选择和权衡。

另外，需要说的一点是，静态内容其实我们还可以借助cdn去处理，而不是单单的依靠nginx或者apache去处理。

补充（2016年12月17日 上午11：24）

现在到了比较大一些的互联网公司，也看了京东的用法（可以看开涛的nginx+lua系列http://www.iteye.com/blogs/subjects/nginx-lua）。

lvs+nginx的使用中，nginx还可以作为一个中间环节来减小后端tomcat的服务压力，以及做一些业务切换、分流、前置缓存的功能。














 lvs与nginx区别  http://4593973.blog.51cto.com/4583973/1419072

    lvs和nginx都可以用作多机负载方案，他们各有优缺点，在生产环境中需要好好分析实际情况并加以利用。
    一、lvs的优势：
    1.抗负载能力强，因为lvs工作方式的逻辑是非常简单的，而且工作再网络层第4层，仅作请求分发用，没有流量，所以在效率上基本不需要太过考虑。lvs一般很少出现故障，即使出现故障一般也是其他地方（如内存、CPU等）出现问题导致lvs出现问题。
    2.配置性地，这通常是一大劣势同时也是一大优势，因为没有太多的可配置的选项，所以除了增减服务器，并不需要经常去触碰它，大大减少了人为出错的几率。
    3.工作稳定，因为其本省抗负载能力很强，所以稳定性高也是顺理成章的事，另外各种lvs都有完整的双机热备方案，所以一点不用担心均衡器本身会出什么问题，节点出现故障的话，lvs会自动判别，所以系统整体式非常稳定的。
    4.无流量，lvs仅仅分发请求，而流量并不从它本身出去，所以可以利用它这点来做一些线路分流之用。没有流量同时也保住了均衡器的IO性能不会受到大流量的影响。
    5.lvs基本上能支持所有应用，因为绿色工作在第4层，所以它可以对几乎所有应用做负载均衡，包括http、数据库、聊天室等。
    另外：lvs也不是完全能判别节点故障的，比如在wlc分配方式下，集群里有一个节点没有配置vip，会使整个集群不能使用，这时使用wrr分配方式则会丢掉一台机器。目前这个问题还在进一步测试中。所以用lvs也得多多当心为妙。
    
    二、nginx和lvs作对比的结果：
    1.nginx工作在网络的第7层，所以它可以针对http应用本身来做分流策略，比如针对域名、目录结构等，相比之下lvs并不具备这样的功能，所以nginx单凭这点可以利用的场合就远多于lvs了；但nginx有用的这些功能使其可调整度要高于lvs，所以经常要去触碰触碰，由lvs的第2条优点来看，触碰多了，人为出现问题的几率也就会大。
    2.nginx对网络的依赖较小，理论上只要ping得通，网页访问正常，nginx就能连得通，nginx同时还能区分内外网，如果是同时拥有内外网的节点，就相当于单机拥有了备份线路；lvs就比较依赖于网络环境，目前来看服务器在同一网段内并且lvs使用direct方式分流，效果较能得到保证。另外注意，lvs需要向托管商至少申请多于一个ip来做visual ip，貌似是不能用本省的ip来做VIP的。要做好lvs管理员，确实得跟进学习很多有关网络通信方面的知识，就不再是一个http那么简单了。
    3.nginx安装和配置比较简单，测试起来也很方便，因为它基本能把错误用日志打印出来。lvs的安装和配置、测试就要花比较长的时间，因为同上所述，lvs对网络依赖性比较大，很多时候不能配置成功都是因为网络问题而不是配置问题，出了问题要解决也相应的会麻烦的多。
    4.nginx也同样能承受很高负载且稳定，但负载度很稳定度差lvs还有几个等级：nginx处理所有流量所以受限于机器IO和配置；本身的bug也还是难以避免的；nginx没有现成的双机热备方案，所以跑在单机上还是风险比较大，单机上的事情全都很难说。
    5.nginx可以检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点。目前lvs中ldirectd也能支持针对服务器内部的情况来监控，但lvs的原理使其不能重发请求。重发请求这点，比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，nginx会把上传切到另一台服务器重新处理，而lvs就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而恼火。
    6.nginx对请求的异步处理可以帮助节点服务器减轻负载，键入使用Apache直接对外服务，那么出现很多的窄带链接时Apache服务器将会占用大量内存而不能释放，使用多于一个nginx做Apache代理的话，这些窄带链接会被nginx挡住，Apache上就不会堆积过多的请求，这样就减少了相当多的内存占用。这点使用squid也有相同的作用，即使squid本身配置为不缓存，对Apache还是有很大帮助你的。lvs没有这些功能，也就无法能比较。
    7.nginx能支持http和Email（Email的功能估计比较少人用），lvs所支持的应用在这点上会比nginx更过。
    在使用上，一般最前端所采取的的策略应是lvs，也就是dns的指向应为lvs均衡器，lvs的优点另它非常适合做这个任务。
    重要的ip地址，最好交由lvs托管，比如数据库的ip、webservice服务器的ip等等，这些ip地址随着时间推移，使用面会越来越大，如果更换ip则故障会接踵而来。所以将这些重要ip交给lvs托管式最为稳妥的，这样做的唯一缺点是需要VIP数量会比较多。
    nginx可以作为lvs节点机器使用，一是可以利用nginx的功能，二是可以利用nginx的性能。当然这一层面也可以直接使用squid，squid的功能方面就比nginx弱不少，性能上也有所逊色于nginx。
    nginx也可以作为中层代理使用，这一层面nginx基本上无对手，唯一可以撼动nginx的就只有lighttpd了，不过lighttpd目前还没有能做到nginx完全的功能，配置也不那么清晰易读。另外，中层代理的ip也是重要的，所以中层代理业拥有一个VIP和lvs是最完美的方案了。
    nginx也可以作为网页静态服务器。

    具体的应用还得具体分析，如果是比较小的网站（日pv<1000万），用nginx就完全可以了，如果机器也不少，可以用dns轮询，lvs所耗费的机器还是比较多的；大型网站或者重要的服务，机器不发愁的时候要多多考虑利用lvs。

    说明：
    使用nginx+keepalived实现负载均衡，解决单点与高流量并发问题。为什么要用nginx而不用lvs？
    7个理由：
    1.高并发连接：官方测试能够支撑5万并发连接，在实际生产环境中跑到2——3万并发连接数。
    2.内存消耗少：在3万并发连接数下，开启的10个nginx进程才消耗150M内存（150*10=150M）。
    3.配置文件非常简单：风格跟程序一样通俗易懂。
    4.成本低廉：nginx为开源软件，可以免费使用。而购买F5 big-ip、netscaler等硬件负载均衡交换机则需要十多万至几十万人民币。
            （使用nginx做七层负载均衡的理由？）
    5.支持rewrite重写规则：能够根据域名、url的不同，将http请求分到不同的后端服务器群组。
    6.内置的健康检查功能：如果nginx proxy后端的某台web服务器宕机了，不会影响前端访问。
    7.节省带宽：支持gzip压缩，可以添加浏览器本地缓存的header头。

    进一步说明：
    keepalived是linux下面实现vrrp备份路由的高可靠性运行件。基于keepalived设计的服务模式能够真正做到主服务器和备份服务器故障时ip瞬间无缝交接。
    nginx是基于linux2.6内核中epoll模型http服务器，与Apache进程派生模式不同的是nginx进程基于master+slave多进程模型，自身具有非常稳定的子进程管理功能。在master进程分配模式下，master进程永远不进行业务处理，只是进行任务分发，从而达到master进程的存活高可靠性，slave进程所有的业务信号都由主进程发出，slave进城所有的超时任务都会被master终止，属于阻塞式人物模型。
    服务器ip存活检测是由keepalived自己本身完成的，将2台服务器配置成keepalived互为主辅关系，任意一方机器故障对方都能够将ip接管过去。
    keepalived的服务器ip通过其配置文件进行管理，依靠其自身的进程去确定服务器的存活状态，如果在需要对服务器进程在线维护的情况下，只需要停掉被维护机器的keepalived服务进程，另外一台服务器就能够接管该台服务器的所有应用。




